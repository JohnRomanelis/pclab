[
  {
    "objectID": "02.4_datasets_modelnet40_fewshot.html",
    "href": "02.4_datasets_modelnet40_fewshot.html",
    "title": "ModelNet40-Few Shot",
    "section": "",
    "text": "The code to load the ModelNet40 dataset comes from the DGCNN repo. There are some alternations, so that it can store and load the data from a custom path. Also there is an option to load only a specific class.\n\nsource\n\nModelNetFewShot\n\n ModelNetFewShot (path, split='train', way=5, shot=10, fold=0,\n                  transforms=[])\n\nDataset to access the few shot classification data for the ModelNet40 dataset.\n\ntransforms=[RandomPointKeep(1024), RandomPointDropout(), UnitSphereNormalization(), AnisotropicScale(), ToTensor()]\ndataset = ModelNetFewShot(path, 'train', way=5, shot=10, fold=5, transforms=transforms)\nlen(dataset)\n\n50",
    "crumbs": [
      "Datasets",
      "ModelNet40-Few Shot"
    ]
  },
  {
    "objectID": "experiments/diffusion_pointcloud.html",
    "href": "experiments/diffusion_pointcloud.html",
    "title": "Point Cloud Diffusion",
    "section": "",
    "text": "# Dataset-related\nfrom pclab.datasets.modelnet import ModelNet40\nfrom pclab.transforms import *\nfrom pclab.learner import *\nfrom torch.utils.data import DataLoader\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Visualization\nfrom pclab.utils import quick_vis\n\n# Other\nfrom functools import partial\nfrom pclab.models.pointnet import LinearLayer\n\n# Lightning\nimport pytorch_lightning as pl\n\nJupyter environment detected. Enabling Open3D WebVisualizer.\n[Open3D INFO] WebRTC GUI backend enabled.\n[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n\n\nINFO - 2023-11-29 11:34:48,958 - instantiator - Created a temporary directory at /tmp/tmpokpiiryu\nINFO - 2023-11-29 11:34:48,959 - instantiator - Writing /tmp/tmpokpiiryu/_remote_module_non_scriptable.py"
  },
  {
    "objectID": "experiments/diffusion_pointcloud.html#imports",
    "href": "experiments/diffusion_pointcloud.html#imports",
    "title": "Point Cloud Diffusion",
    "section": "",
    "text": "# Dataset-related\nfrom pclab.datasets.modelnet import ModelNet40\nfrom pclab.transforms import *\nfrom pclab.learner import *\nfrom torch.utils.data import DataLoader\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Visualization\nfrom pclab.utils import quick_vis\n\n# Other\nfrom functools import partial\nfrom pclab.models.pointnet import LinearLayer\n\n# Lightning\nimport pytorch_lightning as pl\n\nJupyter environment detected. Enabling Open3D WebVisualizer.\n[Open3D INFO] WebRTC GUI backend enabled.\n[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n\n\nINFO - 2023-11-29 11:34:48,958 - instantiator - Created a temporary directory at /tmp/tmpokpiiryu\nINFO - 2023-11-29 11:34:48,959 - instantiator - Writing /tmp/tmpokpiiryu/_remote_module_non_scriptable.py"
  },
  {
    "objectID": "experiments/diffusion_pointcloud.html#dataset",
    "href": "experiments/diffusion_pointcloud.html#dataset",
    "title": "Point Cloud Diffusion",
    "section": "Dataset",
    "text": "Dataset\n\npath = \"/home/ioannis/Desktop/programming/data\"\n\ntransforms = [RandomPointKeep(1024), UnitSphereNormalization(), AnisotropicScale(), ToTensor()]\n\ntrain_dataset = ModelNet40(path, 2048, 'train', transforms=transforms, category=0)\nvalid_dataset = ModelNet40(path, 2048, 'test' , transforms=transforms, category=0)\n\ntrain_dl, valid_dl = map(partial(DataLoader, batch_size=32, shuffle=True, num_workers=8, drop_last=True), (train_dataset, valid_dataset))\nlen(train_dl)\n\n19"
  },
  {
    "objectID": "experiments/diffusion_pointcloud.html#point-autoencoder",
    "href": "experiments/diffusion_pointcloud.html#point-autoencoder",
    "title": "Point Cloud Diffusion",
    "section": "Point AutoEncoder",
    "text": "Point AutoEncoder\n\nclass Encoder(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.ln1 = LinearLayer(in_features, out_features)\n        \n    def forward(self, pc): # expecting `pc` to be of shape `BxFxN`, where `F` is the number of input features and `N` is the number of points in the point cloud\n        return self.ln1(pc)\n\n\nclass Decoder(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.ln1 = LinearLayer(in_features, out_features)\n    \n    def forward(self, pc): # expecting `pc` to be of shape `BxFxN`, where `F` is the number of input features and `N` is the number of points in the point cloud\n        return self.ln1(pc)\n\n\nclass PointAutoEncoder(pl.LightningModule):\n    \n    def __init__(self, point_dim, latent_dim, noise_scale=0.2):\n        super().__init__()\n        self.point_dim, self.latent_dim = point_dim, latent_dim\n        self.noise_scale = noise_scale\n        \n        self.encoder = Encoder(point_dim, latent_dim)\n        self.decoder = Decoder(latent_dim, point_dim)\n        \n        self.loss_fn = nn.MSELoss()\n        \n    def forward(self, pc):\n        pass\n    \n    def training_step(self, batch, batch_idx):\n        \n        pc, _ = batch\n        \n        # add noise on the point clouds to move points to new locations\n        noise = self.noise_scale * torch.randn_line(pc).to(pc.device)\n        pc = pc + noise\n        \n        # encoding points to latent space\n        latent_repr = self.encoder(pc)\n        \n        # decoding points back to point space\n        pc_rec = self.decoder(latent_repr)\n        \n        # MSE loss\n        loss = self.loss_fn(pc_rec, pc)\n        \n        return loss\n        \n    def validation_step(self, batch, batch_idx):\n        \n        pc, _ = batch\n        \n        # add noise on the point clouds to move points to new locations\n        noise = self.noise_scale * torch.randn_line(pc).to(pc.device)\n        pc = pc + noise\n        \n        # encoding points to latent space\n        latent_repr = self.encoder(pc)\n        \n        # decoding points back to point space\n        pc_rec = self.decoder(latent_repr)\n        \n        # MSE loss\n        loss = self.loss_fn(pc_rec, pc)\n    \n    def configure_optimizers(self):\n        opt = optim.Adam(params=self.parameters(), lr=1e-3, weight_decay=0.05)\n        return opt\n\n\nTrain the PointAutoEncoder\n\nmodel = PointAutoEncoder(point_dim=3, latent_dim=64)\n\ntrainer = pl.Trainer(\n    accelerator='gpu', \n    auto_lr_find=True)\n\ntr\n\n/home/ioannis/anaconda3/envs/vvrenv/lib/python3.8/site-packages/torch/cuda/__init__.py:83: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484810403/work/c10/cuda/CUDAFunctions.cpp:109.)\n  return torch._C._cuda_getDeviceCount() &gt; 0\n\n\nMisconfigurationException: No supported gpu backend found!\n\n\n\ntrainer = pl.Trainer(\n    accelerator='gpu',\n    devices=1,\n    max_epochs=5\n)\n\n\ntorch.cuda.is_available()\n\nTrue"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utils",
    "section": "",
    "text": "source\n\nto_device\n\n to_device (x, device='cpu')\n\n\nsource\n\n\nDataLoaders\n\n DataLoaders (*dls)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\npc_to_o3d\n\n pc_to_o3d (pc)\n\nturn a point cloud, represented as a np.array or torch.tensor to an Open3D.geometry.PointCloud\n\n\n\n\nDetails\n\n\n\n\npc\npoint cloud as np.array or torch.tensor\n\n\n\n\nsource\n\n\nquick_vis\n\n quick_vis (pc)\n\n\n\n\n\nDetails\n\n\n\n\npc\npoint cloud as np.array or torch.tensor\n\n\n\n\n\nLoss Functions\n\nsource\n\ncal_loss\n\n cal_loss (pred, gold, smoothing=True)\n\nCalculate cross entropy loss, apply label smoothing if needed.",
    "crumbs": [
      "Utils"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "pclab",
    "section": "",
    "text": "This file will become your README and also the index of your documentation.",
    "crumbs": [
      "pclab"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "pclab",
    "section": "Install",
    "text": "Install\npip install pclab",
    "crumbs": [
      "pclab"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "pclab",
    "section": "How to use",
    "text": "How to use\nFill me in please! Donâ€™t forget code examples:\n\n1+1\n\n2",
    "crumbs": [
      "pclab"
    ]
  },
  {
    "objectID": "02.2_datasets_scanobjectnn.html",
    "href": "02.2_datasets_scanobjectnn.html",
    "title": "ScanObjectNN",
    "section": "",
    "text": "source\n\nScanObjectNN\n\n ScanObjectNN (path:str, subset:str, difficulty:str='normal',\n               background:bool=True, transforms=[])\n\nA dataset structure to load the ScanObjectNN dataset\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr\n\npath of the h5_files folder (.../ScanObjectNN/h5_files/)\n\n\nsubset\nstr\n\ntrain or test set\n\n\ndifficulty\nstr\nnormal\nreturn the variant based on the difficulty (options: normal, hard)\n\n\nbackground\nbool\nTrue\nreturn the versions with the background or without\n\n\ntransforms\nlist\n[]\ntransforms to apply to the data for augmentation\n\n\n\n\ndataset = ScanObjectNN(path, 'train')\nprint(f'Variant with {len(dataset)} samples, with shape {dataset[0][0].shape}')\n\nVariant with 2309 samples, with shape (2048, 3)\n\n\n\ntransforms = [FirstKPointsKeep(1024), AnisotropicScale(), UnitSphereNormalization()]\ndataset = ScanObjectNN(path, 'train', background=False, transforms=transforms)\nprint(f'Variant with {len(dataset)} samples, with shape {dataset[0][0].shape}')\n\nVariant with 2309 samples, with shape (1024, 3)\n\n\n\ndataset = ScanObjectNN(path, 'train', background=False, difficulty='hard')\nprint(f'Variant with {len(dataset)} samples, with shape {dataset[0][0].shape}')\n\nVariant with 11416 samples, with shape (2048, 3)\n\n\n\nsource\n\n\nScanObjectNN_normal\n\n ScanObjectNN_normal (path:str, subset:str, transforms=[])\n\nA dataset structure to load the normal variant of the ScanObjectNN dataset.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr\n\npath of the folder that contains the .h5 files.\n\n\nsubset\nstr\n\ntrain or test set\n\n\ntransforms\nlist\n[]\ntransfors to apply to the data for augmentation\n\n\n\n\ndataset = ScanObjectNN_normal(path, 'train')\nprint(f'Variant with {len(dataset)} samples, with shape {dataset[0][0].shape}')\n\nVariant with 2309 samples, with shape (2048, 3)\n\n\n\nsource\n\n\nScanObjectNN_hardest\n\n ScanObjectNN_hardest (path:str, subset:str, transforms=[])\n\nA dataset structure to load the hardest variant of the ScanObjectNN dataset\n\ntransforms = [FirstKPointsKeep(1024), AnisotropicScale(), UnitSphereNormalization()]\ndataset = ScanObjectNN_hardest(path, 'train', transforms=transforms)\nprint(f'Variant with {len(dataset)} samples, with shape {dataset[0][0].shape}')\n\nVariant with 11416 samples, with shape (1024, 3)",
    "crumbs": [
      "Datasets",
      "ScanObjectNN"
    ]
  },
  {
    "objectID": "models.pointnet.html",
    "href": "models.pointnet.html",
    "title": "PointNet",
    "section": "",
    "text": "NOTE: THIS PAGE NEEDS FORMATING AND ADDING GENERAL FUNCTIONALITY\n\nsource\n\n\nLinearLayer\n\n LinearLayer (in_channels, out_channels, fc=False, use_norm=True,\n              use_relu=True)\n\nA linear layer that can be either used as a shared-mlp or as a block in a fully connected network\n\nsource\n\n\nTNET\n\n TNET (in_features:int)\n\nThe TNET network that is part of the PointNet architecture\n\nsource\n\n\nPointNetCore\n\n PointNetCore (point_features:int)\n\nAs PointNet core we consider the remains of the classification network if we remove the classification head.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\npoint_features\nint\nvalues per point, usually the x,y,z coordinates\n\n\n\n\nsource\n\n\nPointNetCLSHead\n\n PointNetCLSHead (in_channels:int, num_classes:int)\n\nThe classification head used in PointNet\n\n\n\n\nType\nDetails\n\n\n\n\nin_channels\nint\nfeature dimension of the latent space vector\n\n\nnum_classes\nint\nnumber of classes\n\n\n\n\nsource\n\n\nPointNetCLS\n\n PointNetCLS (point_features:int, num_classes:int)\n\nPointNet variant for classification\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\npoint_features\nint\nvalues per point, usually the x,y,z coordinates\n\n\nnum_classes\nint\nnumber of classes\n\n\n\n\nsource\n\n\nPointNetSeg\n\n PointNetSeg (in_channels:int=3)\n\nA variant of PointNet that outputs a feature vector for each point in the point cloud\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nin_channels\nint\n3\nvalues per point, usually the x,y,z coordinates",
    "crumbs": [
      "Models",
      "PointNet"
    ]
  },
  {
    "objectID": "02.1_datasets_modelnet40.html",
    "href": "02.1_datasets_modelnet40.html",
    "title": "ModelNet40",
    "section": "",
    "text": "The code to load the ModelNet40 dataset comes from the DGCNN repo. There are some alternations, so that it can store and load the data from a custom path. Also there is an option to load only a specific class.\n\nsource\n\ndownload\n\n download (path:str=None)\n\nA functions that downloads the ModelNet40 data, if not already downloaded, in the specified path\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr\nNone\nif path is None, it will download the data on the current dir, under the data subfolder.\n\n\n\n\nsource\n\n\nload_data\n\n load_data (partition:str, path:str=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npartition\nstr\n\ntrain or test partition\n\n\npath\nstr\nNone\n\n\n\n\nThe labels in the downloaded file are represented as integers. Therefore, it is necessary to establish a mapping that associates these integers with the actual labels of the categories.\n\nm40_odered_labels[0], m40_odered_labels[8], m40_odered_labels[24]\n\n('airplane', 'chair', 'person')\n\n\n\nm40_cat2int['airplane'], m40_cat2int['chair'], m40_cat2int['person']\n\n(0, 8, 24)\n\n\n\nsource\n\n\nModelNet40\n\n ModelNet40 (path:str, num_points:int, partition:str='train',\n             transforms=[], category=-1)\n\nA ModelNet40 class is necessary for loading and accessing the data, and it inherits from the torch.utils.Dataset class.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr\n\npath of the dataset\n\n\nnum_points\nint\n\nnumber of points\n\n\npartition\nstr\ntrain\nwhich partition to use (train or test)\n\n\ntransforms\nlist\n[]\nthe transforms to apply on each sample\n\n\ncategory\nint\n-1\nselect a specific category of the dataset either by index or by name. By default returns samples from all 40 categories.\n\n\n\nExamples\nLoad the full dataset:\n\ndataset = ModelNet40(path, 1024, 'train')\nlen(dataset)\n\n9840\n\n\nLoad the airplane category using the category label:\n\ndataset = ModelNet40(path, 1024, 'train', category = 'airplane')\nlen(dataset)\n\n625\n\n\nLoad the airplane category using the category index:\n\ndataset = ModelNet40(path, 1024, 'train', category = 0)\nlen(dataset)\n\n625\n\n\nLoad a dataset with custom transforms:\n\ntransforms = [RandomPointDropout(), RandomShuffle(), UnitSphereNormalization(), AnisotropicScale(), ToTensor()]\ndataset = ModelNet40(path, 1024, 'train', transforms=transforms)\nlen(dataset)\n\n9840\n\n\n\nsource\n\n\nget_modelnet\n\n get_modelnet (path, version='standard', return_dls=True, batch_size=32)\n\nGet a version of ModelNet from a predefined set of versions, for faster coding\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\n\n\n\n\n\nversion\nstr\nstandard\nwhich version to return\n\n\nreturn_dls\nbool\nTrue\nreturn dataloaders instead of dataset\n\n\nbatch_size\nint\n32\nthe batch_size to use if returning a dataloader\n\n\n\nSo to get a predifined version of ModelNet:\n\ntrain_loader, valid_loader = get_modelnet(path)",
    "crumbs": [
      "Datasets",
      "ModelNet40"
    ]
  },
  {
    "objectID": "learner.html",
    "href": "learner.html",
    "title": "Learner",
    "section": "",
    "text": "NOTE: THIS PAGE NEEDS FORMATING AND ADDING GENERAL FUNCTIONALITY\n\nsource\n\n\nto_cpu\n\n to_cpu (x)\n\n\nsource\n\n\nCancelEpochException\nCommon base class for all non-exit exceptions.\n\nsource\n\n\nCancelBatchException\nCommon base class for all non-exit exceptions.\n\nsource\n\n\nCancelFitException\nCommon base class for all non-exit exceptions.\n\nsource\n\n\nCallback\n\n Callback ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nrun_cbs\n\n run_cbs (cbs, method_nm, learn=None)\n\n\nsource\n\n\nwith_cbs\n\n with_cbs (nm)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nLearner\n\n Learner (model, dls=(0,), loss_func=&lt;function mse_loss&gt;, lr=0.1,\n          cbs=None, opt_func=&lt;class 'torch.optim.sgd.SGD'&gt;)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nTrainLearner\n\n TrainLearner (model, dls=(0,), loss_func=&lt;function mse_loss&gt;, lr=0.1,\n               cbs=None, opt_func=&lt;class 'torch.optim.sgd.SGD'&gt;)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nBasic Callbacks\n\nsource\n\nTrainCB\n\n TrainCB (n_inp=1)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nLRFinderCB\n\n LRFinderCB (gamma=1.3, max_mult=3)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nMetricsCB\n\n MetricsCB (*ms, **metrics)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nAccuracy\n\n Accuracy (average:Optional[str]='micro', num_classes:Optional[int]=None,\n           k:int=1, device:Optional[torch.device]=None)\n\nThis class is a wrapper for torchval.metrics.MulticlassAccuracy. It receives as input the prediction of the model and the target and: - finds the index of the maximum value in the prediction - aka the class - squizes the target to remove the last dimension\n\nsource\n\n\nDeviceCB\n\n DeviceCB (device='cpu')\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nProgressCB\n\n ProgressCB (plot=False)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nScheduler Callbacks\n\nsource\n\n\nBaseSchedCB\n\n BaseSchedCB (sched)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nBatchSchedCB\n\n BatchSchedCB (sched)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nEpochSchedCB\n\n EpochSchedCB (sched)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nAdding LRFind to the learner\n\nsource\n\n\nshow_doc\n\n show_doc (sym, renderer=None, name:Optional[str]=None, title_level:int=3)\n\nShow signature and docstring for sym\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsym\n\n\nSymbol to document\n\n\nrenderer\nNoneType\nNone\nOptional renderer (defaults to markdown)\n\n\nname\nstr | None\nNone\nOptionally override displayed name of sym\n\n\ntitle_level\nint\n3\nHeading level to use for symbol name\n\n\n\n\n\n\nWeightsAndBiases Logger\n\nsource\n\nWandBLogger\n\n WandBLogger (project, name, config, *ms, **metrics)\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "02.3_datasets_shapenetpart.html",
    "href": "02.3_datasets_shapenetpart.html",
    "title": "ShapeNetPart",
    "section": "",
    "text": "def download_shapenetpart(DATA_DIR):\n    pass"
  },
  {
    "objectID": "02.3_datasets_shapenetpart.html#shapenetpart",
    "href": "02.3_datasets_shapenetpart.html#shapenetpart",
    "title": "ShapeNetPart",
    "section": "",
    "text": "def download_shapenetpart(DATA_DIR):\n    pass"
  },
  {
    "objectID": "transforms.html",
    "href": "transforms.html",
    "title": "Transforms",
    "section": "",
    "text": "source\n\nTransform\n\n Transform ()\n\nA base class for transforms. In case a tuple or a list is the input it will only apply the transforms to the first element and return the others as they are. At children class you have to define a forward method that will perform the transform\n\nsource\n\n\nFirstKPointsKeep\n\n FirstKPointsKeep (num_points:int)\n\nKeep a subset of the original pointcloud containing the K first points\n\n\n\n\nType\nDetails\n\n\n\n\nnum_points\nint\nnumber of points to keep\n\n\n\n\nsource\n\n\nRandomPointKeep\n\n RandomPointKeep (num_points:int)\n\nKeep a random subset of points from the original pointcloud\n\n\n\n\nType\nDetails\n\n\n\n\nnum_points\nint\nnumber of points to keep\n\n\n\n\nsource\n\n\nUnitSphereNormalization\n\n UnitSphereNormalization (return_scale=False)\n\nNormalizes the coordinates of the point cloud, so that all points are inside the unit sphere\n\nsource\n\n\nRandomShuffle\n\n RandomShuffle ()\n\nShuffles the order of the points inside the point cloud\n\nsource\n\n\nAnisotropicScale\n\n AnisotropicScale ()\n\nPerforms a random anisotropic scaling of the point cloud\n\nsource\n\n\nRandomPointDropout\n\n RandomPointDropout (max_dropout_ratio=0.875)\n\nRandomly removes a subset of the point cloud. To keep the shape of the point cloud constant we set all the removed points to have the same coordinates.\n\nsource\n\n\nToTensor\n\n ToTensor ()\n\nTurns a numpy array to a torch tensor",
    "crumbs": [
      "Transforms"
    ]
  },
  {
    "objectID": "experiments/miniai_diffusion_pointcloud.html",
    "href": "experiments/miniai_diffusion_pointcloud.html",
    "title": "Point Cloud Diffusion",
    "section": "",
    "text": "# pclab\nfrom pclab.datasets.modelnet import ModelNet40\nfrom pclab.transforms import *\nfrom pclab.learner import *\nfrom pclab.utils import quick_vis, DataLoaders, to_device\nfrom pclab.models.pointnet import LinearLayer, TNET, feature_transform_regularizer\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom torch.utils.data import DataLoader\n\n# Other\nfrom random import random\nfrom functools import partial\nimport fastcore.all as fc\n\nJupyter environment detected. Enabling Open3D WebVisualizer.\n[Open3D INFO] WebRTC GUI backend enabled.\n[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n\n\nSetting the latent dimension for the whole pipeline:\n\nlatent_dim = 32"
  },
  {
    "objectID": "experiments/miniai_diffusion_pointcloud.html#imports",
    "href": "experiments/miniai_diffusion_pointcloud.html#imports",
    "title": "Point Cloud Diffusion",
    "section": "",
    "text": "# pclab\nfrom pclab.datasets.modelnet import ModelNet40\nfrom pclab.transforms import *\nfrom pclab.learner import *\nfrom pclab.utils import quick_vis, DataLoaders, to_device\nfrom pclab.models.pointnet import LinearLayer, TNET, feature_transform_regularizer\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom torch.utils.data import DataLoader\n\n# Other\nfrom random import random\nfrom functools import partial\nimport fastcore.all as fc\n\nJupyter environment detected. Enabling Open3D WebVisualizer.\n[Open3D INFO] WebRTC GUI backend enabled.\n[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n\n\nSetting the latent dimension for the whole pipeline:\n\nlatent_dim = 32"
  },
  {
    "objectID": "experiments/miniai_diffusion_pointcloud.html#dataset",
    "href": "experiments/miniai_diffusion_pointcloud.html#dataset",
    "title": "Point Cloud Diffusion",
    "section": "Dataset",
    "text": "Dataset\n\npath = \"/home/ioannis/Desktop/programming/data\"\npath = \"/home/vvr/Desktop/vlassisgiannis/new_exps/data\"\n\ntransforms = [RandomPointKeep(1024), UnitSphereNormalization(), AnisotropicScale(), ToTensor()]\n\ntrain_dataset = ModelNet40(path, 2048, 'train', transforms=transforms, category=0)\nvalid_dataset = ModelNet40(path, 2048, 'test' , transforms=transforms, category=0)\n\n\ntrain_dl, valid_dl = map(partial(DataLoader, batch_size=32, shuffle=True, num_workers=8, drop_last=True), (train_dataset, valid_dataset))\n\ndls = DataLoaders(train_dl, valid_dl)"
  },
  {
    "objectID": "experiments/miniai_diffusion_pointcloud.html#point-autoencoder",
    "href": "experiments/miniai_diffusion_pointcloud.html#point-autoencoder",
    "title": "Point Cloud Diffusion",
    "section": "Point AutoEncoder",
    "text": "Point AutoEncoder\n\nclass Encoder(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.ln1 = LinearLayer(in_features, out_features)\n        self.ln2 = LinearLayer(out_features, out_features, use_norm=True, use_relu=False)\n        \n    def forward(self, pc): # expecting `pc` to be of shape `BxFxN`, where `F` is the number of input features and `N` is the number of points in the point cloud\n        return self.ln2(self.ln1(pc))\n\n\nclass Decoder(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.ln1 = LinearLayer(in_features, in_features)\n        self.ln2 = LinearLayer(in_features, out_features, use_norm=False, use_relu=False)\n\n    def forward(self, pc): # expecting `pc` to be of shape `BxFxN`, where `F` is the number of input features and `N` is the number of points in the point cloud\n        return self.ln2(self.ln1(pc))\n\nCreate a model that will contain both the encoder and the decoder subnetworks\n\nclass PointAutoEncoder(nn.Module):\n    \n    def __init__(self, point_dim, latent_dim):\n        super().__init__()\n        self.point_dim, self.latent_dim = point_dim, latent_dim\n        \n        self.encoder = Encoder(point_dim, latent_dim)\n        self.decoder = Decoder(latent_dim, point_dim)\n        \n        self.loss_fn = nn.MSELoss()\n    \n    def encode(self, pc):\n        return self.encoder(pc)\n    \n    def decode(self, latent_pc):\n        return self.decoder(latent_pc)\n    \n    def forward(self, pc):\n        # encoding points to latent space\n        latent_repr = self.encode(pc)\n        \n        # decoding points back to point space\n        pc_rec = self.decode(latent_repr)\n        \n        return pc_rec\n\n\nTraining Callback\n\nAdd noise to the input batch\nCompute the correct loss\n\n\nclass PointAutoEncoderCB(TrainCB):\n    order = 0\n    def __init__(self, noise_scale=0.2):\n        super().__init__()\n        self.noise_scale=noise_scale\n        \n    def before_batch(self, learn):\n        pc, label = learn.batch\n        \n        B = pc.shape[0]     \n        \n        # create noise \n        noise = self.noise_scale * torch.rand_like(pc).to(pc.device)\n        pc = pc + noise \n        \n        learn.batch = pc.permute(0, 2, 1)\n    \n    def predict(self, learn): learn.preds = learn.model(learn.batch)\n    \n    def get_loss(self, learn):\n        learn.loss = learn.loss_func(learn.preds, learn.batch)"
  },
  {
    "objectID": "experiments/miniai_diffusion_pointcloud.html#lrfinder",
    "href": "experiments/miniai_diffusion_pointcloud.html#lrfinder",
    "title": "Point Cloud Diffusion",
    "section": "LRFinder",
    "text": "LRFinder\n\nautoencoder = PointAutoEncoder(point_dim=3, latent_dim=64)\n\n\ncbs = [DeviceCB(),PointAutoEncoderCB()]\nopt_func = partial(optim.Adam, eps=1e-5)\nlearn = Learner(autoencoder, dls, nn.MSELoss(), cbs=cbs, lr=1e-4, opt_func=opt_func)\n\n\nlearn.lr_find(max_mult=3)"
  },
  {
    "objectID": "experiments/miniai_diffusion_pointcloud.html#training",
    "href": "experiments/miniai_diffusion_pointcloud.html#training",
    "title": "Point Cloud Diffusion",
    "section": "Training",
    "text": "Training\nSetting up the scheduler\n\nlr = 0.1 #0.05\nepochs = 10\ntotal_steps = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps = total_steps)\n\nSetting the callbacks\n\ncbs = [PointAutoEncoderCB(), DeviceCB(), ProgressCB(plot=True), MetricsCB(), BatchSchedCB(sched)]\n\n\nautoencoder = PointAutoEncoder(point_dim=3, latent_dim=latent_dim)\nlearn = Learner(autoencoder, dls, nn.MSELoss(), lr=lr, cbs=cbs, opt_func=opt_func)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\n0.118\n0\ntrain\n\n\n\n\n0.118\n0\ntrain\n\n\n0.025\n0\neval\n\n\n0.009\n1\ntrain\n\n\n0.023\n1\neval\n\n\n0.003\n2\ntrain\n\n\n0.009\n2\neval\n\n\n0.004\n3\ntrain\n\n\n0.002\n3\neval\n\n\n0.002\n4\ntrain\n\n\n0.002\n4\neval\n\n\n0.002\n5\ntrain\n\n\n0.002\n5\neval\n\n\n0.002\n6\ntrain\n\n\n0.001\n6\neval\n\n\n0.001\n7\ntrain\n\n\n0.001\n7\neval\n\n\n0.001\n8\ntrain\n\n\n0.000\n8\neval\n\n\n0.001\n9\ntrain\n\n\n0.000\n9\neval"
  },
  {
    "objectID": "experiments/miniai_diffusion_pointcloud.html#create-a-network-that-will-hundle-the-denoising",
    "href": "experiments/miniai_diffusion_pointcloud.html#create-a-network-that-will-hundle-the-denoising",
    "title": "Point Cloud Diffusion",
    "section": "Create a network that will hundle the denoising",
    "text": "Create a network that will hundle the denoising\nThis is usually a UNET but in this case it will be a PointNet-like network.\n\nclass Denoiser(nn.Module):\n    def __init__(self, feature_dim):\n        super().__init__()\n        \n        self.TNETin = TNET(feature_dim)\n        self.ln1 = LinearLayer(feature_dim, feature_dim)\n        self.TNEThid = TNET(feature_dim)\n        self.ln2 = LinearLayer(feature_dim, feature_dim * 2)\n        \n        self.ln_out1 = LinearLayer(feature_dim * 4, feature_dim)\n        self.ln_out2 = LinearLayer(feature_dim, feature_dim, use_norm=False, use_relu=False)\n        \n    def forward(self, latent):\n        \n        self.m1 = self.TNETin(latent)\n        latent = self.m1 @ latent\n        \n        latent = self.ln1(latent)\n        self.m2 = self.TNEThid(latent)\n        latent = self.m2 @ latent\n        latent = self.ln2(latent)\n        \n        B, F, N = latent.shape\n        latent_glob = latent.max(dim=-1, keepdim=True)[0]\n        latent_glob = latent_glob.repeat(1, 1, N)\n        \n        latent = torch.cat([latent, latent_glob], dim=1)\n        \n        latent = self.ln_out1(latent)\n        latent = self.ln_out2(latent)\n        \n        return latent\n    \n    @property\n    def regularization_loss(self):\n        return feature_transform_regularizer(self.m1) + feature_transform_regularizer(self.m2)\n\n\nsample = next(iter(dls.valid))[0].permute(0, 2, 1).cuda()\nsample.shape\n\ntorch.Size([32, 3, 1024])\n\n\n\n#model.eval()\nlatent = autoencoder.encode(sample)\nlatent.shape\n\ntorch.Size([32, 32, 1024])\n\n\n\nunet = Denoiser(latent_dim).cuda()\n\n\nlatent_d = unet(latent)\nlatent_d.shape\n\ntorch.Size([32, 32, 1024])"
  },
  {
    "objectID": "experiments/miniai_diffusion_pointcloud.html#adding-noise",
    "href": "experiments/miniai_diffusion_pointcloud.html#adding-noise",
    "title": "Point Cloud Diffusion",
    "section": "Adding noise:",
    "text": "Adding noise:\n\nCalculating the noise to go to the next timestep:\n\\(x_t = \\sqrt{1 - \\beta_t} x_{t-1} + \\sqrt{\\beta_t} \\epsilon, \\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\\)\n\n\nCalculating the noise to go at a specific timestep:\n\\(x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, \\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\\)\n\ndef noisify(x0,        # the original data\n            á¾±,         # values across all timesteps\n            n_steps):  # total number of steps \n    device = x0.device\n    n = len(x0)\n    # selecting a random timestep\n    t = torch.randint(0, n_steps, (n,), dtype=torch.long)\n    # generating noise from a normal distribution\n    Îµ = torch.randn(x0.shape, device=device)\n    # selecting the value to of á¾± based on the random step\n    á¾±_t = á¾±[t].reshape(-1, 1, 1).to(device)\n    # adding the noise to the sample\n    xt = á¾±_t.sqrt()*x0 + (1-á¾±_t).sqrt()*Îµ\n    #return (xt, t.to(device)), Îµ\n    return xt, Îµ\n\n\n\nTest the process of adding noise.\n\n# Creating the noise stats for a forward diffusion process\nbeta_min = 0.0001\nbeta_max = 0.002\nn_steps = 1000\nbeta = torch.linspace(beta_min, beta_max, n_steps)\nÎ± = 1. - beta\ná¾± = torch.cumprod(Î±, dim=0)\n\n\n# adding noise to the latent\nnoisy_latent = noisify(latent, á¾±, n_steps)\nnoisy_latent[0].shape\n\ntorch.Size([32, 32, 1024])\n\n\n\n# use the decoder to reconstrust the point cloud\nrec_noisy_sample = autoencoder.decode(noisy_latent[0])\nquick_vis(rec_noisy_sample.permute(0, 2, 1)[0])"
  },
  {
    "objectID": "experiments/miniai_diffusion_pointcloud.html#backward-process",
    "href": "experiments/miniai_diffusion_pointcloud.html#backward-process",
    "title": "Point Cloud Diffusion",
    "section": "Backward process",
    "text": "Backward process\nGiven a noise prediction we want to go for the step t+1 to step t.\nThe idea is that we predict the noise that would go directly from the timestep t-1 to the timestep 0. Then we try to predict the timestep t-1 and add noise to go to step t."
  },
  {
    "objectID": "experiments/miniai_diffusion_pointcloud.html#ddpm-callback",
    "href": "experiments/miniai_diffusion_pointcloud.html#ddpm-callback",
    "title": "Point Cloud Diffusion",
    "section": "DDPM callback",
    "text": "DDPM callback\n\n@torch.no_grad()\ndef sample(model, sz, alpha, alphabar, sigma, n_steps):\n    device = next(model.parameters()).device\n    x_t = torch.randn(sz, device=device)\n    preds = []\n    for t in reversed(range(n_steps)):\n        t_batch = torch.full((x_t.shape[0],), t, device=device, dtype=torch.long)\n        z = (torch.randn(x_t.shape) if t &gt; 0 else torch.zeros(x_t.shape)).to(device)\n        á¾±_t1 = alphabar[t-1] if t &gt; 0 else torch.tensor(1)\n        bÌ„_t = 1 - alphabar[t]\n        bÌ„_t1 = 1 - á¾±_t1\n        x_0_hat = ((x_t - bÌ„_t.sqrt() * learn.model((x_t)))/alphabar[t].sqrt()).clamp(-1,1)\n        x_t = x_0_hat * á¾±_t1.sqrt()*(1-alpha[t])/bÌ„_t + x_t * alpha[t].sqrt()*bÌ„_t1/bÌ„_t + sigma[t]*z\n        preds.append(x_t.cpu())\n    return preds\n\n\nclass DDPMCB(Callback):\n    order = DeviceCB.order + 1\n    def __init__(self, n_steps, beta_min, beta_max):\n        super().__init__()\n        fc.store_attr()\n        self.beta = torch.linspace(self.beta_min, self.beta_max, self.n_steps)\n        self.Î± = 1. - self.beta\n        self.á¾± = torch.cumprod(self.Î±, dim=0)\n        self.Ïƒ = self.beta.sqrt()\n        \n    def before_batch(self, learn): \n        # use the encoder to get the latent space vector\n        with torch.no_grad():\n            latent = autoencoder.encode(learn.batch[0].permute(0, 2, 1))\n        # add noise to the encoded latent\n        learn.batch = noisify(latent, self.á¾±, self.n_steps)\n    \n    def after_loss(self, learn):\n        learn.loss = learn.loss + 0.001 * learn.model.regularization_loss\n    \n    def sample(self, model, sz): return sample(model, sz, self.Î±, self.á¾±, self.Ïƒ, self.n_steps)\n\n\nddpm_cb = DDPMCB(n_steps=1000, beta_min=0.0001, beta_max=0.02)\n\n\nunet = Denoiser(latent_dim)\n\nautoencoder.eval()\n\nlr = 0.001\nepochs = 400\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\ncbs = [ddpm_cb, DeviceCB(), ProgressCB(plot=True), MetricsCB(), BatchSchedCB(sched)]\n\nopt_func = partial(optim.Adam, eps=1e-5)\n\nlearn = TrainLearner(unet, dls, nn.MSELoss(), lr=lr, cbs=cbs, opt_func=opt_func)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\n1.190\n0\ntrain\n\n\n\n\n1.190\n0\ntrain\n\n\n1.127\n0\neval\n\n\n1.097\n1\ntrain\n\n\n1.104\n1\neval\n\n\n1.063\n2\ntrain\n\n\n1.030\n2\neval\n\n\n1.034\n3\ntrain\n\n\n0.982\n3\neval\n\n\n1.013\n4\ntrain\n\n\n1.002\n4\neval\n\n\n0.992\n5\ntrain\n\n\n0.980\n5\neval\n\n\n0.970\n6\ntrain\n\n\n0.964\n6\neval\n\n\n0.955\n7\ntrain\n\n\n0.947\n7\neval\n\n\n0.936\n8\ntrain\n\n\n0.934\n8\neval\n\n\n0.919\n9\ntrain\n\n\n0.922\n9\neval\n\n\n0.910\n10\ntrain\n\n\n0.897\n10\neval\n\n\n0.894\n11\ntrain\n\n\n0.897\n11\neval\n\n\n0.885\n12\ntrain\n\n\n0.886\n12\neval\n\n\n0.873\n13\ntrain\n\n\n0.886\n13\neval\n\n\n0.862\n14\ntrain\n\n\n0.850\n14\neval\n\n\n0.847\n15\ntrain\n\n\n0.834\n15\neval\n\n\n0.836\n16\ntrain\n\n\n0.823\n16\neval\n\n\n0.825\n17\ntrain\n\n\n0.830\n17\neval\n\n\n0.815\n18\ntrain\n\n\n0.803\n18\neval\n\n\n0.808\n19\ntrain\n\n\n0.805\n19\neval\n\n\n0.795\n20\ntrain\n\n\n0.791\n20\neval\n\n\n0.784\n21\ntrain\n\n\n0.780\n21\neval\n\n\n0.775\n22\ntrain\n\n\n0.759\n22\neval\n\n\n0.764\n23\ntrain\n\n\n0.749\n23\neval\n\n\n0.755\n24\ntrain\n\n\n0.745\n24\neval\n\n\n0.747\n25\ntrain\n\n\n0.742\n25\neval\n\n\n0.737\n26\ntrain\n\n\n0.749\n26\neval\n\n\n0.728\n27\ntrain\n\n\n0.743\n27\neval\n\n\n0.720\n28\ntrain\n\n\n0.709\n28\neval\n\n\n0.712\n29\ntrain\n\n\n0.695\n29\neval\n\n\n0.699\n30\ntrain\n\n\n0.674\n30\neval\n\n\n0.695\n31\ntrain\n\n\n0.718\n31\neval\n\n\n0.682\n32\ntrain\n\n\n0.722\n32\neval\n\n\n0.677\n33\ntrain\n\n\n0.668\n33\neval\n\n\n0.673\n34\ntrain\n\n\n0.669\n34\neval\n\n\n0.658\n35\ntrain\n\n\n0.638\n35\neval\n\n\n0.643\n36\ntrain\n\n\n0.682\n36\neval\n\n\n0.635\n37\ntrain\n\n\n0.658\n37\neval\n\n\n0.637\n38\ntrain\n\n\n0.609\n38\neval\n\n\n0.631\n39\ntrain\n\n\n0.605\n39\neval\n\n\n0.608\n40\ntrain\n\n\n0.632\n40\neval\n\n\n0.596\n41\ntrain\n\n\n0.560\n41\neval\n\n\n0.581\n42\ntrain\n\n\n0.643\n42\neval\n\n\n0.579\n43\ntrain\n\n\n0.588\n43\neval\n\n\n0.597\n44\ntrain\n\n\n0.582\n44\neval\n\n\n0.574\n45\ntrain\n\n\n0.541\n45\neval\n\n\n0.551\n46\ntrain\n\n\n0.518\n46\neval\n\n\n0.542\n47\ntrain\n\n\n0.681\n47\neval\n\n\n0.547\n48\ntrain\n\n\n0.642\n48\neval\n\n\n0.573\n49\ntrain\n\n\n0.646\n49\neval\n\n\n0.555\n50\ntrain\n\n\n0.870\n50\neval\n\n\n0.536\n51\ntrain\n\n\n0.634\n51\neval\n\n\n0.500\n52\ntrain\n\n\n0.513\n52\neval\n\n\n0.497\n53\ntrain\n\n\n0.499\n53\neval\n\n\n0.497\n54\ntrain\n\n\n0.507\n54\neval\n\n\n0.494\n55\ntrain\n\n\n0.545\n55\neval\n\n\n0.485\n56\ntrain\n\n\n0.531\n56\neval\n\n\n0.481\n57\ntrain\n\n\n0.587\n57\neval\n\n\n0.459\n58\ntrain\n\n\n0.459\n58\neval\n\n\n0.450\n59\ntrain\n\n\n0.503\n59\neval\n\n\n0.448\n60\ntrain\n\n\n0.501\n60\neval\n\n\n0.439\n61\ntrain\n\n\n0.428\n61\neval\n\n\n0.423\n62\ntrain\n\n\n0.514\n62\neval\n\n\n0.426\n63\ntrain\n\n\n0.530\n63\neval\n\n\n0.412\n64\ntrain\n\n\n0.429\n64\neval\n\n\n0.416\n65\ntrain\n\n\n0.472\n65\neval\n\n\n0.595\n66\ntrain\n\n\n0.739\n66\neval\n\n\n0.492\n67\ntrain\n\n\n0.518\n67\neval\n\n\n0.438\n68\ntrain\n\n\n0.480\n68\neval\n\n\n0.411\n69\ntrain\n\n\n0.523\n69\neval\n\n\n0.406\n70\ntrain\n\n\n0.474\n70\neval\n\n\n0.398\n71\ntrain\n\n\n0.512\n71\neval\n\n\n0.391\n72\ntrain\n\n\n0.490\n72\neval\n\n\n0.392\n73\ntrain\n\n\n0.463\n73\neval\n\n\n0.386\n74\ntrain\n\n\n0.447\n74\neval\n\n\n0.373\n75\ntrain\n\n\n0.481\n75\neval\n\n\n0.359\n76\ntrain\n\n\n0.510\n76\neval\n\n\n0.352\n77\ntrain\n\n\n0.363\n77\neval\n\n\n0.354\n78\ntrain\n\n\n0.610\n78\neval\n\n\n0.362\n79\ntrain\n\n\n0.436\n79\neval\n\n\n0.334\n80\ntrain\n\n\n0.512\n80\neval\n\n\n0.318\n81\ntrain\n\n\n0.551\n81\neval\n\n\n0.321\n82\ntrain\n\n\n0.477\n82\neval\n\n\n0.317\n83\ntrain\n\n\n0.448\n83\neval\n\n\n0.324\n84\ntrain\n\n\n0.434\n84\neval\n\n\n0.323\n85\ntrain\n\n\n0.313\n85\neval\n\n\n0.318\n86\ntrain\n\n\n0.430\n86\neval\n\n\n0.318\n87\ntrain\n\n\n0.426\n87\neval\n\n\n0.297\n88\ntrain\n\n\n0.349\n88\neval\n\n\n0.330\n89\ntrain\n\n\n0.427\n89\neval\n\n\n0.303\n90\ntrain\n\n\n0.337\n90\neval\n\n\n0.293\n91\ntrain\n\n\n0.412\n91\neval\n\n\n0.296\n92\ntrain\n\n\n0.308\n92\neval\n\n\n0.289\n93\ntrain\n\n\n0.409\n93\neval\n\n\n0.308\n94\ntrain\n\n\n0.487\n94\neval\n\n\n0.293\n95\ntrain\n\n\n0.376\n95\neval\n\n\n0.288\n96\ntrain\n\n\n0.330\n96\neval\n\n\n0.286\n97\ntrain\n\n\n0.398\n97\neval\n\n\n0.286\n98\ntrain\n\n\n0.461\n98\neval\n\n\n0.293\n99\ntrain\n\n\n0.325\n99\neval\n\n\n0.272\n100\ntrain\n\n\n0.406\n100\neval\n\n\n0.272\n101\ntrain\n\n\n0.298\n101\neval\n\n\n0.302\n102\ntrain\n\n\n0.564\n102\neval\n\n\n0.297\n103\ntrain\n\n\n0.465\n103\neval\n\n\n0.284\n104\ntrain\n\n\n0.298\n104\neval\n\n\n0.254\n105\ntrain\n\n\n0.276\n105\neval\n\n\n0.262\n106\ntrain\n\n\n0.364\n106\neval\n\n\n0.308\n107\ntrain\n\n\n0.565\n107\neval\n\n\n0.380\n108\ntrain\n\n\n1.017\n108\neval\n\n\n0.350\n109\ntrain\n\n\n0.682\n109\neval\n\n\n0.285\n110\ntrain\n\n\n0.396\n110\neval\n\n\n0.259\n111\ntrain\n\n\n0.337\n111\neval\n\n\n0.251\n112\ntrain\n\n\n0.328\n112\neval\n\n\n0.269\n113\ntrain\n\n\n0.348\n113\neval\n\n\n0.247\n114\ntrain\n\n\n0.341\n114\neval\n\n\n0.244\n115\ntrain\n\n\n0.369\n115\neval\n\n\n0.247\n116\ntrain\n\n\n0.295\n116\neval\n\n\n0.232\n117\ntrain\n\n\n0.315\n117\neval\n\n\n0.220\n118\ntrain\n\n\n0.274\n118\neval\n\n\n0.249\n119\ntrain\n\n\n0.424\n119\neval\n\n\n0.242\n120\ntrain\n\n\n0.329\n120\neval\n\n\n0.231\n121\ntrain\n\n\n0.312\n121\neval\n\n\n0.208\n122\ntrain\n\n\n0.249\n122\neval\n\n\n0.234\n123\ntrain\n\n\n0.305\n123\neval\n\n\n0.221\n124\ntrain\n\n\n0.257\n124\neval\n\n\n0.225\n125\ntrain\n\n\n0.302\n125\neval\n\n\n0.220\n126\ntrain\n\n\n0.292\n126\neval\n\n\n0.229\n127\ntrain\n\n\n0.346\n127\neval\n\n\n0.202\n128\ntrain\n\n\n0.247\n128\neval\n\n\n0.213\n129\ntrain\n\n\n0.392\n129\neval\n\n\n0.215\n130\ntrain\n\n\n0.316\n130\neval\n\n\n0.212\n131\ntrain\n\n\n0.275\n131\neval\n\n\n0.208\n132\ntrain\n\n\n0.281\n132\neval\n\n\n0.207\n133\ntrain\n\n\n0.190\n133\neval\n\n\n0.217\n134\ntrain\n\n\n0.307\n134\neval\n\n\n0.212\n135\ntrain\n\n\n0.225\n135\neval\n\n\n0.194\n136\ntrain\n\n\n0.245\n136\neval\n\n\n0.201\n137\ntrain\n\n\n0.222\n137\neval\n\n\n0.202\n138\ntrain\n\n\n0.308\n138\neval\n\n\n0.204\n139\ntrain\n\n\n0.223\n139\neval\n\n\n0.211\n140\ntrain\n\n\n0.308\n140\neval\n\n\n0.229\n141\ntrain\n\n\n0.379\n141\neval\n\n\n0.221\n142\ntrain\n\n\n0.236\n142\neval\n\n\n0.198\n143\ntrain\n\n\n0.412\n143\neval\n\n\n0.200\n144\ntrain\n\n\n0.232\n144\neval\n\n\n0.194\n145\ntrain\n\n\n0.274\n145\neval\n\n\n0.201\n146\ntrain\n\n\n0.241\n146\neval\n\n\n0.192\n147\ntrain\n\n\n0.197\n147\neval\n\n\n0.206\n148\ntrain\n\n\n0.254\n148\neval\n\n\n0.220\n149\ntrain\n\n\n0.278\n149\neval\n\n\n0.199\n150\ntrain\n\n\n0.354\n150\neval\n\n\n0.197\n151\ntrain\n\n\n0.223\n151\neval\n\n\n0.196\n152\ntrain\n\n\n0.241\n152\neval\n\n\n0.199\n153\ntrain\n\n\n0.198\n153\neval\n\n\n0.235\n154\ntrain\n\n\n0.943\n154\neval\n\n\n0.257\n155\ntrain\n\n\n0.345\n155\neval\n\n\n0.256\n156\ntrain\n\n\n0.313\n156\neval\n\n\n0.252\n157\ntrain\n\n\n0.494\n157\neval\n\n\n0.220\n158\ntrain\n\n\n0.284\n158\neval\n\n\n0.209\n159\ntrain\n\n\n0.239\n159\neval\n\n\n0.191\n160\ntrain\n\n\n0.263\n160\neval\n\n\n0.210\n161\ntrain\n\n\n0.240\n161\neval\n\n\n0.201\n162\ntrain\n\n\n0.234\n162\neval\n\n\n0.204\n163\ntrain\n\n\n0.230\n163\neval\n\n\n0.182\n164\ntrain\n\n\n0.179\n164\neval\n\n\n0.174\n165\ntrain\n\n\n0.192\n165\neval\n\n\n0.174\n166\ntrain\n\n\n0.205\n166\neval\n\n\n0.171\n167\ntrain\n\n\n0.193\n167\neval\n\n\n0.178\n168\ntrain\n\n\n0.191\n168\neval\n\n\n0.182\n169\ntrain\n\n\n0.195\n169\neval\n\n\n0.165\n170\ntrain\n\n\n0.237\n170\neval\n\n\n0.223\n171\ntrain\n\n\n0.298\n171\neval\n\n\n0.184\n172\ntrain\n\n\n0.259\n172\neval\n\n\n0.183\n173\ntrain\n\n\n0.214\n173\neval\n\n\n0.172\n174\ntrain\n\n\n0.213\n174\neval\n\n\n0.172\n175\ntrain\n\n\n0.170\n175\neval\n\n\n0.178\n176\ntrain\n\n\n0.209\n176\neval\n\n\n0.170\n177\ntrain\n\n\n0.224\n177\neval\n\n\n0.174\n178\ntrain\n\n\n0.194\n178\neval\n\n\n0.172\n179\ntrain\n\n\n0.237\n179\neval\n\n\n0.181\n180\ntrain\n\n\n0.172\n180\neval\n\n\n0.165\n181\ntrain\n\n\n0.201\n181\neval\n\n\n0.161\n182\ntrain\n\n\n0.164\n182\neval\n\n\n0.167\n183\ntrain\n\n\n0.142\n183\neval\n\n\n0.163\n184\ntrain\n\n\n0.188\n184\neval\n\n\n0.172\n185\ntrain\n\n\n0.163\n185\neval\n\n\n0.167\n186\ntrain\n\n\n0.133\n186\neval\n\n\n0.167\n187\ntrain\n\n\n0.149\n187\neval\n\n\n0.170\n188\ntrain\n\n\n0.167\n188\neval\n\n\n0.161\n189\ntrain\n\n\n0.195\n189\neval\n\n\n0.167\n190\ntrain\n\n\n0.211\n190\neval\n\n\n0.153\n191\ntrain\n\n\n0.166\n191\neval\n\n\n0.177\n192\ntrain\n\n\n0.203\n192\neval\n\n\n0.165\n193\ntrain\n\n\n0.164\n193\neval\n\n\n0.169\n194\ntrain\n\n\n0.213\n194\neval\n\n\n0.158\n195\ntrain\n\n\n0.193\n195\neval\n\n\n0.170\n196\ntrain\n\n\n0.191\n196\neval\n\n\n0.154\n197\ntrain\n\n\n0.183\n197\neval\n\n\n0.161\n198\ntrain\n\n\n0.220\n198\neval\n\n\n0.181\n199\ntrain\n\n\n0.207\n199\neval\n\n\n0.208\n200\ntrain\n\n\n0.302\n200\neval\n\n\n0.248\n201\ntrain\n\n\n0.281\n201\neval\n\n\n0.206\n202\ntrain\n\n\n0.241\n202\neval\n\n\n0.170\n203\ntrain\n\n\n0.181\n203\neval\n\n\n0.171\n204\ntrain\n\n\n0.209\n204\neval\n\n\n0.193\n205\ntrain\n\n\n0.299\n205\neval\n\n\n0.188\n206\ntrain\n\n\n0.257\n206\neval\n\n\n0.166\n207\ntrain\n\n\n0.252\n207\neval\n\n\n0.174\n208\ntrain\n\n\n0.262\n208\neval\n\n\n0.162\n209\ntrain\n\n\n0.183\n209\neval\n\n\n0.172\n210\ntrain\n\n\n0.219\n210\neval\n\n\n0.167\n211\ntrain\n\n\n0.170\n211\neval\n\n\n0.162\n212\ntrain\n\n\n0.173\n212\neval\n\n\n0.165\n213\ntrain\n\n\n0.193\n213\neval\n\n\n0.168\n214\ntrain\n\n\n0.268\n214\neval\n\n\n0.161\n215\ntrain\n\n\n0.243\n215\neval\n\n\n0.159\n216\ntrain\n\n\n0.172\n216\neval\n\n\n0.156\n217\ntrain\n\n\n0.153\n217\neval\n\n\n0.249\n218\ntrain\n\n\n0.412\n218\neval\n\n\n0.200\n219\ntrain\n\n\n0.305\n219\neval\n\n\n0.189\n220\ntrain\n\n\n0.266\n220\neval\n\n\n0.186\n221\ntrain\n\n\n0.197\n221\neval\n\n\n0.176\n222\ntrain\n\n\n0.205\n222\neval\n\n\n0.172\n223\ntrain\n\n\n0.164\n223\neval\n\n\n0.162\n224\ntrain\n\n\n0.190\n224\neval\n\n\n0.166\n225\ntrain\n\n\n0.156\n225\neval\n\n\n0.164\n226\ntrain\n\n\n0.140\n226\neval\n\n\n0.156\n227\ntrain\n\n\n0.176\n227\neval\n\n\n0.161\n228\ntrain\n\n\n0.166\n228\neval\n\n\n0.169\n229\ntrain\n\n\n0.158\n229\neval\n\n\n0.156\n230\ntrain\n\n\n0.138\n230\neval\n\n\n0.159\n231\ntrain\n\n\n0.155\n231\neval\n\n\n0.156\n232\ntrain\n\n\n0.176\n232\neval\n\n\n0.143\n233\ntrain\n\n\n0.166\n233\neval\n\n\n0.135\n234\ntrain\n\n\n0.149\n234\neval\n\n\n0.145\n235\ntrain\n\n\n0.165\n235\neval\n\n\n0.148\n236\ntrain\n\n\n0.204\n236\neval\n\n\n0.138\n237\ntrain\n\n\n0.146\n237\neval\n\n\n0.139\n238\ntrain\n\n\n0.134\n238\neval\n\n\n0.146\n239\ntrain\n\n\n0.171\n239\neval\n\n\n0.145\n240\ntrain\n\n\n0.144\n240\neval\n\n\n0.144\n241\ntrain\n\n\n0.113\n241\neval\n\n\n0.146\n242\ntrain\n\n\n0.179\n242\neval\n\n\n0.133\n243\ntrain\n\n\n0.151\n243\neval\n\n\n0.147\n244\ntrain\n\n\n0.234\n244\neval\n\n\n0.152\n245\ntrain\n\n\n0.193\n245\neval\n\n\n0.143\n246\ntrain\n\n\n0.152\n246\neval\n\n\n0.135\n247\ntrain\n\n\n0.162\n247\neval\n\n\n0.142\n248\ntrain\n\n\n0.099\n248\neval\n\n\n0.143\n249\ntrain\n\n\n0.100\n249\neval\n\n\n0.139\n250\ntrain\n\n\n0.097\n250\neval\n\n\n0.142\n251\ntrain\n\n\n0.141\n251\neval\n\n\n0.122\n252\ntrain\n\n\n0.120\n252\neval\n\n\n0.131\n253\ntrain\n\n\n0.110\n253\neval\n\n\n0.144\n254\ntrain\n\n\n0.123\n254\neval\n\n\n0.146\n255\ntrain\n\n\n0.134\n255\neval\n\n\n0.137\n256\ntrain\n\n\n0.166\n256\neval\n\n\n0.142\n257\ntrain\n\n\n0.122\n257\neval\n\n\n0.144\n258\ntrain\n\n\n0.178\n258\neval\n\n\n0.145\n259\ntrain\n\n\n0.156\n259\neval\n\n\n0.163\n260\ntrain\n\n\n0.190\n260\neval\n\n\n0.155\n261\ntrain\n\n\n0.226\n261\neval\n\n\n0.154\n262\ntrain\n\n\n0.118\n262\neval\n\n\n0.136\n263\ntrain\n\n\n0.102\n263\neval\n\n\n0.149\n264\ntrain\n\n\n0.148\n264\neval\n\n\n0.143\n265\ntrain\n\n\n0.186\n265\neval\n\n\n0.160\n266\ntrain\n\n\n0.177\n266\neval\n\n\n0.141\n267\ntrain\n\n\n0.187\n267\neval\n\n\n0.141\n268\ntrain\n\n\n0.184\n268\neval\n\n\n0.151\n269\ntrain\n\n\n0.146\n269\neval\n\n\n0.131\n270\ntrain\n\n\n0.103\n270\neval\n\n\n0.136\n271\ntrain\n\n\n0.159\n271\neval\n\n\n0.128\n272\ntrain\n\n\n0.119\n272\neval\n\n\n0.132\n273\ntrain\n\n\n0.107\n273\neval\n\n\n0.133\n274\ntrain\n\n\n0.126\n274\neval\n\n\n0.143\n275\ntrain\n\n\n0.114\n275\neval\n\n\n0.136\n276\ntrain\n\n\n0.127\n276\neval\n\n\n0.141\n277\ntrain\n\n\n0.135\n277\neval\n\n\n0.137\n278\ntrain\n\n\n0.148\n278\neval\n\n\n0.137\n279\ntrain\n\n\n0.161\n279\neval\n\n\n0.143\n280\ntrain\n\n\n0.146\n280\neval\n\n\n0.136\n281\ntrain\n\n\n0.143\n281\neval\n\n\n0.136\n282\ntrain\n\n\n0.102\n282\neval\n\n\n0.133\n283\ntrain\n\n\n0.130\n283\neval\n\n\n0.130\n284\ntrain\n\n\n0.138\n284\neval\n\n\n0.139\n285\ntrain\n\n\n0.137\n285\neval\n\n\n0.135\n286\ntrain\n\n\n0.186\n286\neval\n\n\n0.131\n287\ntrain\n\n\n0.127\n287\neval\n\n\n0.126\n288\ntrain\n\n\n0.131\n288\neval\n\n\n0.128\n289\ntrain\n\n\n0.102\n289\neval\n\n\n0.137\n290\ntrain\n\n\n0.166\n290\neval\n\n\n0.137\n291\ntrain\n\n\n0.125\n291\neval\n\n\n0.133\n292\ntrain\n\n\n0.114\n292\neval\n\n\n0.135\n293\ntrain\n\n\n0.131\n293\neval\n\n\n0.119\n294\ntrain\n\n\n0.136\n294\neval\n\n\n0.130\n295\ntrain\n\n\n0.154\n295\neval\n\n\n0.122\n296\ntrain\n\n\n0.096\n296\neval\n\n\n0.135\n297\ntrain\n\n\n0.112\n297\neval\n\n\n0.143\n298\ntrain\n\n\n0.185\n298\neval\n\n\n0.140\n299\ntrain\n\n\n0.160\n299\neval\n\n\n0.140\n300\ntrain\n\n\n0.123\n300\neval\n\n\n0.138\n301\ntrain\n\n\n0.136\n301\neval\n\n\n0.133\n302\ntrain\n\n\n0.097\n302\neval\n\n\n0.121\n303\ntrain\n\n\n0.146\n303\neval\n\n\n0.129\n304\ntrain\n\n\n0.137\n304\neval\n\n\n0.121\n305\ntrain\n\n\n0.127\n305\neval\n\n\n0.129\n306\ntrain\n\n\n0.116\n306\neval\n\n\n0.130\n307\ntrain\n\n\n0.127\n307\neval\n\n\n0.128\n308\ntrain\n\n\n0.148\n308\neval\n\n\n0.137\n309\ntrain\n\n\n0.111\n309\neval\n\n\n0.135\n310\ntrain\n\n\n0.115\n310\neval\n\n\n0.140\n311\ntrain\n\n\n0.120\n311\neval\n\n\n0.122\n312\ntrain\n\n\n0.129\n312\neval\n\n\n0.142\n313\ntrain\n\n\n0.123\n313\neval\n\n\n0.133\n314\ntrain\n\n\n0.165\n314\neval\n\n\n0.144\n315\ntrain\n\n\n0.148\n315\neval\n\n\n0.149\n316\ntrain\n\n\n0.196\n316\neval\n\n\n0.136\n317\ntrain\n\n\n0.153\n317\neval\n\n\n0.127\n318\ntrain\n\n\n0.130\n318\neval\n\n\n0.136\n319\ntrain\n\n\n0.109\n319\neval\n\n\n0.133\n320\ntrain\n\n\n0.151\n320\neval\n\n\n0.122\n321\ntrain\n\n\n0.131\n321\neval\n\n\n0.126\n322\ntrain\n\n\n0.126\n322\neval\n\n\n0.127\n323\ntrain\n\n\n0.114\n323\neval\n\n\n0.128\n324\ntrain\n\n\n0.114\n324\neval\n\n\n0.124\n325\ntrain\n\n\n0.133\n325\neval\n\n\n0.125\n326\ntrain\n\n\n0.099\n326\neval\n\n\n0.120\n327\ntrain\n\n\n0.096\n327\neval\n\n\n0.123\n328\ntrain\n\n\n0.109\n328\neval\n\n\n0.132\n329\ntrain\n\n\n0.132\n329\neval\n\n\n0.120\n330\ntrain\n\n\n0.116\n330\neval\n\n\n0.120\n331\ntrain\n\n\n0.095\n331\neval\n\n\n0.137\n332\ntrain\n\n\n0.157\n332\neval\n\n\n0.135\n333\ntrain\n\n\n0.159\n333\neval\n\n\n0.135\n334\ntrain\n\n\n0.162\n334\neval\n\n\n0.124\n335\ntrain\n\n\n0.177\n335\neval\n\n\n0.133\n336\ntrain\n\n\n0.110\n336\neval\n\n\n0.139\n337\ntrain\n\n\n0.140\n337\neval\n\n\n0.130\n338\ntrain\n\n\n0.127\n338\neval\n\n\n0.125\n339\ntrain\n\n\n0.110\n339\neval\n\n\n0.130\n340\ntrain\n\n\n0.145\n340\neval\n\n\n0.138\n341\ntrain\n\n\n0.160\n341\neval\n\n\n0.128\n342\ntrain\n\n\n0.130\n342\neval\n\n\n0.127\n343\ntrain\n\n\n0.136\n343\neval\n\n\n0.129\n344\ntrain\n\n\n0.124\n344\neval\n\n\n0.128\n345\ntrain\n\n\n0.127\n345\neval\n\n\n0.125\n346\ntrain\n\n\n0.154\n346\neval\n\n\n0.128\n347\ntrain\n\n\n0.114\n347\neval\n\n\n0.125\n348\ntrain\n\n\n0.124\n348\neval\n\n\n0.124\n349\ntrain\n\n\n0.093\n349\neval\n\n\n0.125\n350\ntrain\n\n\n0.090\n350\neval\n\n\n0.123\n351\ntrain\n\n\n0.112\n351\neval\n\n\n0.125\n352\ntrain\n\n\n0.096\n352\neval\n\n\n0.127\n353\ntrain\n\n\n0.123\n353\neval\n\n\n0.127\n354\ntrain\n\n\n0.092\n354\neval\n\n\n0.132\n355\ntrain\n\n\n0.116\n355\neval\n\n\n0.129\n356\ntrain\n\n\n0.119\n356\neval\n\n\n0.128\n357\ntrain\n\n\n0.135\n357\neval\n\n\n0.125\n358\ntrain\n\n\n0.090\n358\neval\n\n\n0.124\n359\ntrain\n\n\n0.105\n359\neval\n\n\n0.123\n360\ntrain\n\n\n0.108\n360\neval\n\n\n0.121\n361\ntrain\n\n\n0.127\n361\neval\n\n\n0.130\n362\ntrain\n\n\n0.106\n362\neval\n\n\n0.124\n363\ntrain\n\n\n0.123\n363\neval\n\n\n0.123\n364\ntrain\n\n\n0.113\n364\neval\n\n\n0.128\n365\ntrain\n\n\n0.125\n365\neval\n\n\n0.124\n366\ntrain\n\n\n0.154\n366\neval\n\n\n0.125\n367\ntrain\n\n\n0.105\n367\neval\n\n\n0.126\n368\ntrain\n\n\n0.120\n368\neval\n\n\n0.131\n369\ntrain\n\n\n0.111\n369\neval\n\n\n0.121\n370\ntrain\n\n\n0.124\n370\neval\n\n\n0.120\n371\ntrain\n\n\n0.096\n371\neval\n\n\n0.123\n372\ntrain\n\n\n0.097\n372\neval\n\n\n0.121\n373\ntrain\n\n\n0.130\n373\neval\n\n\n0.119\n374\ntrain\n\n\n0.107\n374\neval\n\n\n0.126\n375\ntrain\n\n\n0.132\n375\neval\n\n\n0.129\n376\ntrain\n\n\n0.123\n376\neval\n\n\n0.125\n377\ntrain\n\n\n0.106\n377\neval\n\n\n0.126\n378\ntrain\n\n\n0.098\n378\neval\n\n\n0.125\n379\ntrain\n\n\n0.109\n379\neval\n\n\n0.120\n380\ntrain\n\n\n0.144\n380\neval\n\n\n0.123\n381\ntrain\n\n\n0.104\n381\neval\n\n\n0.117\n382\ntrain\n\n\n0.118\n382\neval\n\n\n0.128\n383\ntrain\n\n\n0.120\n383\neval\n\n\n0.133\n384\ntrain\n\n\n0.124\n384\neval\n\n\n0.136\n385\ntrain\n\n\n0.107\n385\neval\n\n\n0.123\n386\ntrain\n\n\n0.137\n386\neval\n\n\n0.127\n387\ntrain\n\n\n0.131\n387\neval\n\n\n0.130\n388\ntrain\n\n\n0.087\n388\neval\n\n\n0.120\n389\ntrain\n\n\n0.126\n389\neval\n\n\n0.124\n390\ntrain\n\n\n0.114\n390\neval\n\n\n0.122\n391\ntrain\n\n\n0.109\n391\neval\n\n\n0.120\n392\ntrain\n\n\n0.106\n392\neval\n\n\n0.132\n393\ntrain\n\n\n0.080\n393\neval\n\n\n0.122\n394\ntrain\n\n\n0.109\n394\neval\n\n\n0.118\n395\ntrain\n\n\n0.127\n395\neval\n\n\n0.121\n396\ntrain\n\n\n0.140\n396\neval\n\n\n0.134\n397\ntrain\n\n\n0.158\n397\neval\n\n\n0.122\n398\ntrain\n\n\n0.152\n398\neval\n\n\n0.124\n399\ntrain\n\n\n0.131\n399\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nsamples = ddpm_cb.sample(learn.model, (1, latent_dim, 1024))\n\n\ngenerated_latent = samples[-1]\ngenerated_latent.shape\n\ntorch.Size([1, 32, 1024])\n\n\n\ngenerated_pc = autoencoder.decode(generated_latent.cuda())\nquick_vis(generated_pc)\n\n\ngenerated_pc.min(), generated_pc.max()\n\n(tensor(-0.5205, device='cuda:0', grad_fn=&lt;MinBackward1&gt;),\n tensor(0.5199, device='cuda:0', grad_fn=&lt;MaxBackward1&gt;))"
  },
  {
    "objectID": "experiments/ddpm_pointcloud.html",
    "href": "experiments/ddpm_pointcloud.html",
    "title": "Dataset",
    "section": "",
    "text": "from pclab.datasets.modelnet import get_modelnet, ModelNet40\nfrom pclab.transforms import *\nfrom torch.utils.data import DataLoader\nfrom pclab.models.pointnet import PointNetSeg\nfrom pclab.utils import quick_vis\n\nimport torch\nimport torch.nn as nn\nfrom functools import partial\nimport torch.optim as optim\n\nimport torch.optim.lr_scheduler as lr_scheduler\nimport matplotlib.pyplot as plt\nimport open3d as o3d\n\nfrom miniai.learner import *\nfrom miniai.sgd import *\n\nJupyter environment detected. Enabling Open3D WebVisualizer.\n[Open3D INFO] WebRTC GUI backend enabled.\n[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n\n\n\npath = \"/home/vvr/Desktop/vlassisgiannis/new_exps/data\"\n#train_dl, valid_dl = get_modelnet(path, batch_size=32)\n\ntransforms = [RandomPointKeep(1024), UnitSphereNormalization(), AnisotropicScale(), ToTensor()]\ntrain_dataset = ModelNet40(path, 2048, 'train', transforms=transforms, category=0)\nvalid_dataset = ModelNet40(path, 2048, 'test' , transforms=transforms, category=0)\n\ntrain_dl, valid_dl = map(partial(DataLoader, batch_size=32, shuffle=True, num_workers=8, drop_last=True), (train_dataset, valid_dataset))\nprint(len(train_dataset))\n\n625\n\n\n\nbatch = next(iter(train_dl))\nprint(batch[0].shape, batch[1].shape)\npc = batch[0]\n\ntorch.Size([32, 1024, 3]) torch.Size([32])\n\n\n\nbeta_min, beta_max = 0.0001, 0.02\nn_steps = 1000\n\nbeta = torch.linspace(beta_min, beta_max, n_steps)\nalpha = 1 - beta\n\nplt.plot(beta)\nplt.plot(alpha)\n\n\n\n\n\n\n\n\n\nalpha_hat = torch.cumprod(alpha, dim=0)\nplt.plot(alpha_hat)\n\n\n\n\n\n\n\n\n\nsigma = beta.sqrt()\nplt.plot(sigma)\n\n\n\n\n\n\n\n\n\ndef euc2sph(pc):\n    \n    r = (pc * pc).sum(-1).sqrt()\n    \n    theta = torch.arccos(pc[..., 2] / r)\n    \n    fi = torch.sign(pc[..., 1]) * torch.arccos(  pc[..., 0] / (pc[..., 0:2] * pc[..., 0:2]).sum(-1).sqrt() )\n    \n    return r, theta , fi\n\ndef sph2euc(pc_shp):\n    r, theta, fi = pc_shp\n    \n    x = r * torch.sin(theta) * torch.cos(fi)\n    y = r * torch.sin(theta) * torch.sin(fi)\n    z = r * torch.cos(theta)\n        \n    return torch.cat([r for r in map(partial(torch.unsqueeze, dim=-1), (x, y, z))], dim=-1)\n\n\nquick_vis(pc[0])\npc_sph = euc2sph(pc)\nr, theta , fi = pc_sph\n\npc_back = sph2euc(pc_sph)\n#quick_vis(pc_back[0])\n\n\ndef plot_sph_dist(pc_sph):\n    \n    plt.figure(figsize=(13, 4))\n    plt.subplot(1, 3, 1)\n    plt.hist(r.view(-1), bins=100)\n    plt.title('r')\n\n    plt.subplot(1, 3, 2)\n    plt.hist(theta.view(-1) / torch.pi * 180, bins=100)\n    plt.title('theta')\n\n    plt.subplot(1, 3, 3)\n    plt.hist(fi.view(-1) / torch.pi * 180, bins=100)\n    plt.title('fi')\n    \n    plt.show()\n\n\nplot_sph_dist(pc_sph)\n\n\n\n\n\n\n\n\n\nnoise = torch.randn(pc.shape)\n#quick_vis(noise[0])\nprint(noise.shape)\n\ntorch.Size([32, 1024, 3])\n\n\n\nnoise_sph = euc2sph(noise)\nr, theta, fi = noise_sph\nplot_sph_dist(noise_sph)\n\n\n\n\n\n\n\n\nWe notice that the angles of the noise generated with the gaussian distribution have the desired distributions. The magnitude of the noise is however 3 times larger than the amplitude of the signal.\n\nr = r / 1.500\nnoise_sph = (r, theta, fi)\nplot_sph_dist(noise_sph)\n\n\n\n\n\n\n\n\n\n# turn the noise back to euclidean space to add it to the point cloud\nnoise = sph2euc(noise_sph)\n#quick_vis(noise[0])\n\n\nnoisy_model = noise + pc\nquick_vis(noisy_model[0])\n\n\nCreating the noise for each step - before batch\n\nscale_factor = 1 #1/1.5\n\n\n# noise \nnoise = scale_factor * torch.randn(batch[0].shape)\n# original sample\nx0 = batch[0]\n\n\n# select a random timestep\nt = torch.randint(0, n_steps, (x0.shape[0], ), dtype=torch.long)\n#t\n\n\nalpha_hat_t = alpha_hat[t].reshape(-1, 1, 1)\n#alpha_hat_t\n\n\nxt = alpha_hat_t.sqrt() * x0 + (1 - alpha_hat_t.sqrt()).sqrt() * noise\n\n\n#quick_vis(xt[0])\n\n\n#quick_vis(xt[1])\n\n\n#quick_vis(batch[0][1])\n\n\nclass DDPMCB(TrainCB):\n    order = DeviceCB.order+1\n    def __init__(self, n_steps, beta_min, beta_max):\n        super().__init__()\n        self.n_steps,self.Î²min,self.Î²max = n_steps,beta_min,beta_max\n        # variance schedule, linearly increased with timestep\n        self.Î² = torch.linspace(self.Î²min, self.Î²max, self.n_steps)\n        self.Î± = 1. - self.Î² \n        self.á¾± = torch.cumprod(self.Î±, dim=0)\n        self.Ïƒ = self.Î².sqrt()\n\n    def predict(self, learn): \n        learn.preds = learn.model(learn.batch[0][0].permute(0, 2, 1))\n    \n    def before_batch(self, learn):\n        device = learn.batch[0].device\n        Îµ = torch.randn(learn.batch[0].shape, device=device)  # noise, x_T\n        x0 = learn.batch[0] # original images, x_0\n        self.á¾± = self.á¾±.to(device)\n        n = x0.shape[0]\n        # select random timesteps\n        t = torch.randint(0, self.n_steps, (n,), device=device, dtype=torch.long)\n        á¾±_t = self.á¾±[t].reshape(-1, 1, 1).to(device)\n        xt = á¾±_t.sqrt()*x0 + (1-á¾±_t).sqrt()*Îµ #noisify the image\n        # input to our model is noisy image and timestep, ground truth is the noise \n        learn.batch = ((xt, t), Îµ)\n    \n    @torch.no_grad()\n    def sample(self, model, sz):\n        device = next(model.parameters()).device\n        x_t = torch.randn(sz, device=device)\n        preds = []\n        for t in reversed(range(self.n_steps)):\n            t_batch = torch.full((x_t.shape[0],), t, device=device, dtype=torch.long)\n            z = (torch.randn(x_t.shape) if t &gt; 0 else torch.zeros(x_t.shape)).to(device)\n            á¾±_t1 = self.á¾±[t-1]  if t &gt; 0 else torch.tensor(1)\n            bÌ„_t = 1 - self.á¾±[t]\n            bÌ„_t1 = 1 - á¾±_t1\n            noise_pred = learn.model(x_t.permute(0, 2, 1))\n            x_0_hat = ((x_t - bÌ„_t.sqrt() * noise_pred)/self.á¾±[t].sqrt()).clamp(-1,1)\n            x0_coeff = á¾±_t1.sqrt()*(1-self.Î±[t])/bÌ„_t\n            xt_coeff = self.Î±[t].sqrt()*bÌ„_t1/bÌ„_t\n            x_t = x_0_hat*x0_coeff + x_t*xt_coeff + self.Ïƒ[t]*z\n            preds.append(x_t.cpu())\n        return preds\n\n\nclass loader_bundle:\n    pass\ndls = loader_bundle\ndls.train = train_dl\ndls.test  = valid_dl\ndls.valid = dls.test\n\n\nmodel = PointNetSeg()\n\n\nlr = 0.001 #4e-3\nepochs = 2\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nddpm_cb = DDPMCB(n_steps=1000, beta_min=0.0001, beta_max=0.02)\ncbs = [ddpm_cb, DeviceCB(), ProgressCB(plot=True), MetricsCB(), BatchSchedCB(sched)]\nlearn = Learner(model, dls, nn.MSELoss(), lr=lr, cbs=cbs, opt_func=optim.Adam)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.561\n0\ntrain\n\n\n0.909\n0\neval\n\n\n0.560\n1\ntrain\n\n\n0.570\n1\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nsamples = ddpm_cb.sample(learn.model, (2, 1024, 3))\n\n\nsamples[0].shape\n\ntorch.Size([2, 1024, 3])\n\n\n\nlast = samples[999][1].cpu()\nquick_vis(last)\n\n\nsamples[999][1].max(), samples[999][1].min()\n\n(tensor(0.9332), tensor(-1.))\n\n\n\nsamples[999][1][..., 0].max(), samples[999][1][..., 1].max(), samples[999][1][..., 2].max()\n\n(tensor(0.7952), tensor(0.8323), tensor(0.9332))\n\n\n\nsamples[999][1][..., 0].min(), samples[999][1][..., 1].min(), samples[999][1][..., 2].min()\n\n(tensor(-1.), tensor(-1.), tensor(-1.))\n\n\n\n\nLoss Functions - Is mse working?"
  }
]