{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea82fa06-0fc5-4381-9c85-5a64906ea3ed",
   "metadata": {},
   "source": [
    "# ScanObjectNN\n",
    "\n",
    "> A Dataset structure for ScanObjectNN dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd77bfa-e1ba-4794-9e7d-d40515a6c8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp datasets/scanobjectnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12ea948-56f0-47d2-9689-83f41ac42b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83416395-fb2c-4276-a5ba-3f505c415732",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from pclab.transforms import *\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccffa7c9-7c72-4786-8d65-f4acff58aafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ScanObjectNN(Dataset):\n",
    "    \"A dataset structure to load the ScanObjectNN dataset\" \n",
    "    def __init__(self, path:str,                # path of the `h5_files` folder (`.../ScanObjectNN/h5_files/`)\n",
    "                       subset:str,              # `train` or `test` set\n",
    "                       difficulty:str='normal', # return the variant based on the difficulty (options: `normal`, `hard`)\n",
    "                       background:bool=True,    # return the versions with the background or without\n",
    "                       transforms=[]):          # transforms to apply to the data for augmentation\n",
    "        self.path, self.subset, self.difficulty, self.background, self.transforms = path, subset, difficulty, background, transforms\n",
    "        \n",
    "        assert subset in ['train', 'test']\n",
    "        assert difficulty in ['normal', 'hard']\n",
    "        \n",
    "        # creating the path based on the background choice\n",
    "        self.path = os.path.join(self.path, 'main_split') if background else os.path.join(self.path, 'main_split_nobg')\n",
    "        \n",
    "        # creating the actual filename based on the the difficulty and the train/test subset\n",
    "        filename = 'objectdataset'\n",
    "        filename = 'training_'+filename if subset=='train' else 'test_'+filename\n",
    "        filename = filename if difficulty=='normal' else filename+'_augmentedrot_scale75'\n",
    "        filename += '.h5'\n",
    "        \n",
    "        # Loading the data\n",
    "        h5 = h5py.File(os.path.join(self.path, filename), 'r')\n",
    "        self.points = np.array(h5['data']).astype(np.float32)\n",
    "        self.labels = np.array(h5['label']).astype(int)\n",
    "        h5.close()\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.points.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        pc = self.points[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        for t in self.transforms:\n",
    "            pc = t(pc)\n",
    "            \n",
    "        return pc, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aa6953-faf0-4f04-b1d0-838f628b152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "path = \"/home/vvr/Desktop/vlassisgiannis/new_exps/data/ScanObjectNN/h5_files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901271fb-ad71-40cd-ad5e-c053d1835d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant with 2309 samples, with shape (2048, 3)\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dataset = ScanObjectNN(path, 'train')\n",
    "print(f'Variant with {len(dataset)} samples, with shape {dataset[0][0].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c713b69-2dfd-444c-bc16-66f7d1224ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant with 2309 samples, with shape (1024, 3)\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "transforms = [FirstKPointsKeep(1024), AnisotropicScale(), UnitSphereNormalization()]\n",
    "dataset = ScanObjectNN(path, 'train', background=False, transforms=transforms)\n",
    "print(f'Variant with {len(dataset)} samples, with shape {dataset[0][0].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3cfe08-3843-4a37-b413-ede44313ff56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant with 11416 samples, with shape (2048, 3)\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dataset = ScanObjectNN(path, 'train', background=False, difficulty='hard')\n",
    "print(f'Variant with {len(dataset)} samples, with shape {dataset[0][0].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb306285-c228-4ce8-82ff-dede38957d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "path = \"/home/vvr/Desktop/vlassisgiannis/new_exps/data/ScanObjectNN/h5_files/main_split\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d35367c-a4bc-4e4d-b4ab-6e8202fcd192",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ScanObjectNN_normal(Dataset):\n",
    "    \"A dataset structure to load the normal variant of the ScanObjectNN dataset.\" \n",
    "    def __init__(self, path:str,         # path of the folder that contains the `.h5` files. \n",
    "                       subset:str,       # `train` or `test` set\n",
    "                       transforms=[]):   # transfors to apply to the data for augmentation\n",
    "        self.path, self.subset, self.transforms = path, subset, transforms\n",
    "        assert subset in ['train', 'test']\n",
    "        \n",
    "        if self.subset == \"train\":  \n",
    "            h5 = h5py.File(os.path.join(self.path, 'training_objectdataset.h5'), 'r')\n",
    "            self.points = np.array(h5['data']).astype(np.float32)\n",
    "            self.labels = np.array(h5['label']).astype(int)\n",
    "            h5.close()\n",
    "        elif self.subset == \"test\":\n",
    "            h5 = h5py.File(os.path.join(self.path, 'test_objectdataset.h5'), 'r')\n",
    "            self.points = np.array(h5['data']).astype(np.float32)\n",
    "            self.labels = np.array(h5['label']).astype(int)\n",
    "            h5.close()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.points.shape[0]\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pt_idxs = np.arange(0, self.points.shape[1]) #2048\n",
    "        if self.subset == 'train':\n",
    "            np.random.shuffle(pt_idxs)\n",
    "\n",
    "        current_points = self.points[idx, pt_idxs].copy()\n",
    "\n",
    "        for t in self.transforms:\n",
    "            current_points = t(current_points)\n",
    "            \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return current_points, np.array([label])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77edeb27-0e2a-41c4-89ff-263db170e575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant with 2309 samples, with shape (2048, 3)\n"
     ]
    }
   ],
   "source": [
    "#| eval:false\n",
    "dataset = ScanObjectNN_normal(path, 'train')\n",
    "print(f'Variant with {len(dataset)} samples, with shape {dataset[0][0].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a15283-d5ef-46e0-b0fa-ed7010510efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "class ScanObjectNN_hardest(ScanObjectNN):\n",
    "    \"A dataset structure to load the hardest variant of the ScanObjectNN dataset\"\n",
    "    \n",
    "    def __init__(self, path:str, subset:str, transforms=[]):\n",
    "        self.path, self.subset, self.transforms = path , subset, transforms\n",
    "        assert subset in ['train', 'test']\n",
    "        \n",
    "        if self.subset == 'train':\n",
    "            h5 = h5py.File(os.path.join(self.path, 'training_objectdataset_augmentedrot_scale75.h5'), 'r')\n",
    "            self.points = np.array(h5['data']).astype(np.float32)\n",
    "            self.labels = np.array(h5['label']).astype(int)\n",
    "            h5.close()\n",
    "        elif self.subset == 'test':\n",
    "            h5 = h5py.File(os.path.join(self.path, 'test_objectdataset_augmentedrot_scale75.h5'), 'r')\n",
    "            self.points = np.array(h5['data']).astype(np.float32)\n",
    "            self.labels = np.array(h5['label']).astype(int)\n",
    "            h5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421fc679-de76-48e4-adcb-e5005dd74d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant with 11416 samples, with shape (1024, 3)\n"
     ]
    }
   ],
   "source": [
    "#| eval:false\n",
    "transforms = [FirstKPointsKeep(1024), AnisotropicScale(), UnitSphereNormalization()]\n",
    "dataset = ScanObjectNN_hardest(path, 'train', transforms=transforms)\n",
    "print(f'Variant with {len(dataset)} samples, with shape {dataset[0][0].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a3234d-311f-4576-bb07-2dc98b4a4b79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
